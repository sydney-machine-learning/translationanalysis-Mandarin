{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (3.8.1)\n",
      "Requirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from nltk) (4.66.1)\n",
      "Requirement already satisfied: click in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from nltk) (2023.12.25)\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 24.0 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#part one: compute the jaccard similarity\n",
    "#https://github.com/VipanchiKatthula/Jaccard_Cosine_Similarity/blob/master/Jaccard_and_Cosine_Similarity.ipynb\n",
    "%pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        English_Verse\n",
      "0   Some people say: Some winners are willing to b...\n",
      "1                     Seeing, he seemed to be flying!\n",
      "2   However, this victory made him a little strang...\n",
      "3                          \"Broken son's grandson Q!\"\n",
      "4   This sentence was heard in A Q's ears.He thoug...\n",
      "5                     \"Women, women! ...\" He thought.\n",
      "6   \".... Monk moves ... women, women! ... woman!\"...\n",
      "7   We can't know when A Q is snoring this night.B...\n",
      "8        That is, we can know that women are harmful.\n",
      "9   Most Chinese men could have been sages, but un...\n",
      "10  Ah Q was originally a positive person. Althoug...\n",
      "11  Who knows that he will be in the year of \"stan...\n",
      "12                         \"Female ...\" Ah Q thought.\n",
      "13  He often pays attention to the woman who think...\n",
      "14  On this day, A Q was springing for a day in th...\n",
      "15  Wu Ma is the only maid in the house of Taiye Z...\n",
      "16  \"My wife didn't eat for two days, because the ...\n",
      "17  \"Woman, Wu Ma, this little lonely,\" Ah Q thought.\n",
      "18  \"Our young grandma is going to have children i...\n",
      "19                          \"Woman ...\" Ah Q thought.\n",
      "20      A Q put down the cigarette pipe and stood up.\n",
      "21                \"Our young grandma ...\" Wu Ma said.\n",
      "22  \"I feel sleepy with you, I and you are sleepy!...\n",
      "23                   It was very silent for a moment.\n",
      "24  \"Ah!\" Wu Ma shook, trembled suddenly, yelled a...\n",
      "25  Ah Q was kneeling on the wall and also froze, ...\n",
      "26                     \"You're reversed, ... you ...\"\n",
      "27  The big bamboo bar split him again.A Q's hands...\n",
      "28  \"Forgot to eight eggs! Xiucai scolded the offi...\n",
      "29  A Q ran into the rice field, standing alone, s...\n",
      "30  When he took off his clothes, he heard that he...\n",
      "31  The young grandmother was dragging Wu's mother...\n",
      "32  \"Come outside, ... don't hide in your own room...\n",
      "33  \"Who doesn't know that you are serious, ... th...\n",
      "34  Wu Ma just cried, clamped some words, but he d...\n",
      "35  Ah Q thought: \"Huh, interesting, this little l...\n",
      "36  A Q sat for a while, his skin was a little bit...\n",
      "37  \"Ah Q, your mother! You even teased by the Zha...\n",
      "38  If Yunyun's lesson, Ah Q naturally had nothing...\n",
      "39  Tomorrow with red candle -a pound of weight -a...\n",
      "40  On the second Zhao Mansion, the Taoist priest ...\n",
      "41  San Ah Q has never been allowed to step into t...\n",
      "42  If the four Wu moms have followed up, Ah Q is ...\n",
      "43  Wu Ah Q is not allowed to ask for workers and ...\n",
      "44  Ah Q naturally agreed, but unfortunately there...\n"
     ]
    }
   ],
   "source": [
    "#load the file\n",
    "df_hsien = pd.read_csv('/Users/wangxuechun/unsw/UNSW-Thesis-/chapter_4_eng_version_hsien.csv')\n",
    "df_google = pd.read_csv('/Users/wangxuechun/unsw/UNSW-Thesis-/chapter_4_eng_version_google.csv')\n",
    "df_yiyun = pd.read_csv('/Users/wangxuechun/unsw/UNSW-Thesis-/chapter_4_eng_version_yiyun.csv')\n",
    "df_google = df_google.drop('Unnamed: 0', axis=1)\n",
    "df_hsien.columns = ['English_Verse']\n",
    "df_yiyun.columns = ['English_Verse']\n",
    "df_google.columns = ['English_Verse']\n",
    "print(df_google)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert lists of lists:\n",
    "def lists_of_lists(lists):\n",
    "    return [item for sublist in lists for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#computation of jaccard similarities\n",
    "def jaccard_similarities(list1, list2):\n",
    "    intersections = len(set(list1).intersection(set(list2)))\n",
    "    unions = len(set(list1).union(set(list2)))\n",
    "    return (intersections/unions)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/wangxuechun/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/wangxuechun/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data cleaning\n",
    "def data_cleaning(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    #remove the punctuation and convert all to the lower case\n",
    "    tokens = [token.lower() for token in tokens if token.isalpha()]\n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_text = [word for word in tokens if word not in stop_words]\n",
    "    return filtered_text\n",
    "\n",
    "\n",
    "hsien_lists = df_hsien['English_Verse'].apply(data_cleaning)\n",
    "google_lists = df_google['English_Verse'].apply(data_cleaning)\n",
    "yiyun_lists = df_yiyun['English_Verse'].apply(data_cleaning)\n",
    "\n",
    "hsien_list = lists_of_lists(hsien_lists)\n",
    "google_list = lists_of_lists(google_lists)\n",
    "yiyun_list = lists_of_lists(yiyun_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.58974358974359 21.49532710280374 26.126126126126124\n"
     ]
    }
   ],
   "source": [
    "#implementation phase\n",
    "js_yg = jaccard_similarities(yiyun_list, google_list)\n",
    "js_hg = jaccard_similarities(hsien_list,google_list)\n",
    "js_hy = jaccard_similarities(hsien_list, yiyun_list)\n",
    "\n",
    "print(js_yg, js_hg, js_hy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#semantic analysis cosine similarity\n",
    "hsien_list      = ','.join(str(v) for v in hsien_list)\n",
    "google_list    = ','.join(str(v) for v in google_list)\n",
    "yiyun_list    = ','.join(str(v) for v in yiyun_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.4.0-1-cp310-cp310-macosx_12_0_arm64.whl (10.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.6/10.6 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting scipy>=1.6.0\n",
      "  Downloading scipy-1.12.0-cp310-cp310-macosx_12_0_arm64.whl (31.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-3.3.0-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: numpy<2.0,>=1.19.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from scikit-learn) (1.26.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from scikit-learn) (1.3.2)\n",
      "Installing collected packages: threadpoolctl, scipy, scikit-learn\n",
      "Successfully installed scikit-learn-1.4.0 scipy-1.12.0 threadpoolctl-3.3.0\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 24.0 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.33609327 0.50282451]\n",
      " [0.33609327 1.         0.30499986]\n",
      " [0.50282451 0.30499986 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "#Calculating Cosine Similarity through TF-IDF\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "doc = [hsien_list, yiyun_list, google_list]\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf = vectorizer.fit_transform(doc)\n",
    "words = vectorizer.get_feature_names_out()\n",
    "similarity_matrix = cosine_similarity(tfidf)\n",
    "\n",
    "print(similarity_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keybert\n",
      "  Downloading keybert-0.8.4.tar.gz (29 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting sentence-transformers>=0.3.8\n",
      "  Downloading sentence_transformers-2.4.0-py3-none-any.whl (149 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.5/149.5 KB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scikit-learn>=0.22.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from keybert) (1.4.0)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from keybert) (1.26.1)\n",
      "Collecting rich>=10.4.0\n",
      "  Downloading rich-13.7.0-py3-none-any.whl (240 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.6/240.6 KB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting markdown-it-py>=2.2.0\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 KB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from rich>=10.4.0->keybert) (2.13.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from scikit-learn>=0.22.2->keybert) (1.12.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from scikit-learn>=0.22.2->keybert) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from scikit-learn>=0.22.2->keybert) (3.3.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.32.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from sentence-transformers>=0.3.8->keybert) (4.37.0)\n",
      "Requirement already satisfied: torch>=1.11.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from sentence-transformers>=0.3.8->keybert) (2.1.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.15.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from sentence-transformers>=0.3.8->keybert) (0.20.3)\n",
      "Requirement already satisfied: Pillow in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from sentence-transformers>=0.3.8->keybert) (10.2.0)\n",
      "Requirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from sentence-transformers>=0.3.8->keybert) (4.66.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers>=0.3.8->keybert) (2023.12.2)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers>=0.3.8->keybert) (2.31.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers>=0.3.8->keybert) (6.0.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers>=0.3.8->keybert) (21.3)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers>=0.3.8->keybert) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers>=0.3.8->keybert) (4.9.0)\n",
      "Collecting mdurl~=0.1\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (3.1.2)\n",
      "Requirement already satisfied: sympy in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (1.12)\n",
      "Requirement already satisfied: networkx in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (3.2.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers>=0.3.8->keybert) (0.4.2)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers>=0.3.8->keybert) (0.15.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers>=0.3.8->keybert) (2023.12.25)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub>=0.15.1->sentence-transformers>=0.3.8->keybert) (3.0.9)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers>=0.3.8->keybert) (2023.7.22)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers>=0.3.8->keybert) (2.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers>=0.3.8->keybert) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers>=0.3.8->keybert) (2.10)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from sympy->torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (1.3.0)\n",
      "Building wheels for collected packages: keybert\n",
      "  Building wheel for keybert (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for keybert: filename=keybert-0.8.4-py3-none-any.whl size=39220 sha256=784a5fcaef80ddffd19bcb54b811b32a23b62bc021b4c2edf33b93b22f1b94e0\n",
      "  Stored in directory: /Users/wangxuechun/Library/Caches/pip/wheels/97/ef/4c/6588bd7072b0cc04225b40e639b991e49ebd4e21fb81f0acee\n",
      "Successfully built keybert\n",
      "Installing collected packages: mdurl, markdown-it-py, rich, sentence-transformers, keybert\n",
      "Successfully installed keybert-0.8.4 markdown-it-py-3.0.0 mdurl-0.1.2 rich-13.7.0 sentence-transformers-2.4.0\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 24.0 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#Keybert - extract keyword\n",
    "%pip install keybert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "982cb2c40a704b378f68e57eed5e71d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "059c4ec3f50440aba5fca229dc1d21c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a6e51ab2ffa4bbb8787e5fe26179746",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea59476a059e48a7b1c877360edf1a82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4a1ec87953646938669825d101b0916",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0850cb6e1ddc44059a69d2c838a89d03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "758743ff45a24a38b31b574f9c2ddbeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7465370d227b485a8d5b70842915e6e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e69ae19821924647ba491b7dafd57b04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cedd19c2595c4cbc84d20f02535330ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8092a437d7c4bb497c899df06df8291",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from keybert import KeyBERT\n",
    "km_model = KeyBERT()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('victory', 0.3793), ('victors', 0.3656), ('surrendered', 0.351), ('triumph', 0.3256), ('rival', 0.3121)], [('fly', 0.4497), ('light', 0.4108), ('elated', 0.2952), ('look', 0.2642), ('ah', 0.1349)], [('nun', 0.4271), ('snored', 0.4183), ('flew', 0.3269), ('finger', 0.2598), ('victory', 0.256)], [('sonless', 0.5798), ('die', 0.2846), ('ah', 0.0955)], [('sonless', 0.2803), ('teachings', 0.2527), ('wife', 0.2264), ('ought', 0.211), ('descendants', 0.206)], [('monk', 0.3812), ('woman', 0.3592), ('paws', 0.3521), ('thought', 0.1073)], [('fingers', 0.2895), ('asleep', 0.2687), ('soft', 0.2443), ('thinking', 0.1925), ('evening', 0.1869)], [('woman', 0.5084), ('menace', 0.4291), ('mankind', 0.3863)], [('shang', 0.4679), ('chinese', 0.3642), ('chan', 0.341), ('cho', 0.3034), ('chin', 0.2992)], [('nuns', 0.4837), ('monks', 0.4243), ('heretics', 0.4008), ('nun', 0.3944), ('morals', 0.3576)], [('nun', 0.5241), ('odiousness', 0.3115), ('headedness', 0.2718), ('woman', 0.2481), ('hateful', 0.2231)], [('woman', 0.5164), ('thought', 0.2514), ('ah', 0.1772)], [('ahl', 0.3862), ('seduce', 0.3473), ('modesty', 0.3339), ('rendezvous', 0.2792), ('odiousness', 0.2486)], [('lamp', 0.3961), ('kitchen', 0.3396), ('light', 0.3245), ('chao', 0.3135), ('rice', 0.2888)], [('chao', 0.4336), ('maidservant', 0.4256), ('amah', 0.4171), ('wu', 0.3607), ('dishes', 0.3343)], [('mistress', 0.5923), ('concubine', 0.5808), ('eaten', 0.3149), ('master', 0.2554), ('hasn', 0.2462)], [('widow', 0.4822), ('amah', 0.3605), ('wu', 0.3338), ('woman', 0.3248), ('little', 0.2036)], [('mistress', 0.5244), ('moon', 0.5012), ('eighth', 0.3147), ('baby', 0.2327), ('young', 0.1943)], [('woman', 0.5133), ('thought', 0.2336), ('ah', 0.1881)], [('stood', 0.5793), ('pipe', 0.504)], [('mistress', 0.4826), ('wu', 0.432), ('amah', 0.431), ('chattered', 0.2638), ('young', 0.2026)], [('rushed', 0.2957), ('sleep', 0.2549), ('suddenly', 0.2062), ('feet', 0.1483), ('forward', 0.1363)], [('silence', 0.6223), ('moment', 0.4166), ('absolute', 0.3557)], [('amah', 0.4273), ('wu', 0.3986), ('ai', 0.3642), ('shrieking', 0.2867), ('sobbing', 0.2689)], [('stood', 0.3623), ('bang', 0.3544), ('kneeling', 0.3302), ('rice', 0.3103), ('bench', 0.3014)], [('dare', 0.4999)], [('bamboo', 0.3451), ('hands', 0.2894), ('pole', 0.2847), ('head', 0.2834), ('shoulders', 0.2825)], [('turtle', 0.4409), ('candidate', 0.3666), ('egg', 0.3659), ('mandarin', 0.3622), ('shouted', 0.3039)], [('weichuang', 0.3475), ('fled', 0.2745), ('grinding', 0.2661), ('grind', 0.258), ('rice', 0.2415)], [('chao', 0.579), ('chen', 0.2848), ('yen', 0.2822), ('tsou', 0.2636), ('mistress', 0.249)], [('mistress', 0.4343), ('wu', 0.3619), ('amah', 0.333), ('servants', 0.3071), ('quarters', 0.1863)], [('brooding', 0.4965), ('room', 0.3141), ('outside', 0.2652), ('stay', 0.1645), ('come', 0.1253)], [('tsou', 0.3876), ('suicide', 0.3656), ('mrs', 0.327), ('woman', 0.2442), ('mustn', 0.1296)], [('muttering', 0.4644), ('wu', 0.4621), ('inaudible', 0.4596), ('wailed', 0.4578), ('amah', 0.3902)], [('chao', 0.4695), ('chen', 0.3648), ('mischief', 0.3594), ('szu', 0.3549), ('bamboo', 0.3278)], [('bailiff', 0.4273), ('cold', 0.323), ('skin', 0.2686), ('goose', 0.2565), ('felt', 0.2536)], [('bailiff', 0.4833), ('chao', 0.4305), ('servants', 0.4019), ('rebel', 0.2614), ('family', 0.1872)], [('curse', 0.4629), ('sleep', 0.4237), ('ve', 0.3374), ('lose', 0.306)], [('bailiff', 0.4782), ('cash', 0.3135), ('abuse', 0.3113), ('pay', 0.2936), ('hat', 0.2588)], [('incense', 0.4833), ('candles', 0.4769), ('chao', 0.3227), ('red', 0.2842), ('weighing', 0.2434)], [('taoist', 0.4431), ('chao', 0.4022), ('priests', 0.3632), ('spirits', 0.317), ('pay', 0.2391)], [('chao', 0.5095), ('household', 0.3215), ('foot', 0.2734), ('ah', 0.1801), ('set', 0.114)], [('wu', 0.5264), ('amah', 0.3905), ('responsible', 0.3017), ('happen', 0.2009), ('ah', 0.1835)], [('wages', 0.4321), ('shirt', 0.3625), ('ah', 0.1916)], [('chao', 0.3744), ('candles', 0.3544), ('incense', 0.3481), ('tattered', 0.342), ('diapers', 0.2413)]]\n"
     ]
    }
   ],
   "source": [
    "#extract keyword from chapter 4\n",
    "df_hsien_texts = df_hsien['English_Verse'].tolist()\n",
    "df_yiyun_texts = df_yiyun['English_Verse'].tolist()\n",
    "df_google_texts = df_google['English_Verse'].tolist()\n",
    "keywords = []\n",
    "for text in df_hsien_texts:\n",
    "    extracted_keywords = km_model.extract_keywords(text, keyphrase_ngram_range=(1, 1), stop_words='english')\n",
    "    keywords.append(extracted_keywords)\n",
    "print(keywords)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
